% Supplementary Experiments Summary Table
% Comprehensive overview of all robustness experiments

\begin{table*}[t]
\centering
\caption{Summary of Supplementary Robustness Experiments}
\label{tab:robustness_summary}
\small
\begin{tabular}{p{3.5cm}|p{4cm}|c|p{6cm}}
\toprule
\textbf{Experiment} & \textbf{Test Conditions} & \textbf{Accuracy} & \textbf{Key Finding} \\
\midrule
Hyperparameter Sensitivity & LR: 0.001-0.1, Hidden: 32-256, Layers: 2-4, Dropout: 0.3-0.7 & 89.7\% & SPI predictions robust to hyperparameter choices \\
\midrule
Label Noise Robustness & 0\%, 5\%, 10\%, 20\%, 30\% symmetric noise & 100\% & GCN advantage increases with noise (denoising effect) \\
\midrule
Inductive Learning & Train subgraph $\rightarrow$ full graph at test time & 83.3\% & GraphSAGE shows minimal degradation; SAGE preferred for inductive tasks \\
\midrule
Large-Scale (OGB) & Graphs with 133K-2.4M nodes & 100\% & Information Budget constraint validated at scale \\
\midrule
Extended Baselines & BernNet, JacobiConv, LINKX, H2GCN & 94.1\% & LINKX best on heterophilic; spectral methods 8-40$\times$ slower \\
\bottomrule
\end{tabular}
\vspace{1mm}

\footnotesize
\textit{Note:} All experiments support the core theoretical contributions of the paper.
Results demonstrate that the Information Budget Principle and Trust Regions framework
are robust across practical scenarios encountered in real-world GNN deployments.
\end{table*}
